{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "s_X0Gqh_-UqM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 5 # set to 5 for base model\n",
        "batch_size_train = 64\n",
        "batch_size_test = 640\n",
        "learning_rate = 0.001\n",
        "momentum = 0.5\n",
        "log_interval = 10\n",
        "\n",
        "random_seed = 1\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.manual_seed(random_seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5Jzaqkb-acN",
        "outputId": "dfacebae-8211-41b2-c362-5fb24509ead5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7804d41898b0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_train, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_test, shuffle=True)"
      ],
      "metadata": {
        "id": "xUpcA1Es-gil"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(2, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 50)\n",
        "        self.fc3 = nn.Linear(50, 10)\n",
        "        self.activation = nn.LeakyReLU(negative_slope=0.01)\n",
        "\n",
        "    def forward(self, x, features=False):\n",
        "        x = self.activation(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = self.activation(F.max_pool2d(self.conv2(x), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return F.log_softmax(x)"
      ],
      "metadata": {
        "id": "MsTQf0xB-jGr"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network = Net().cpu()\n",
        "optimizer = optim.Adam(network.parameters(), lr=1e-3)\n",
        "\n",
        "train_losses = []\n",
        "train_counter = []\n",
        "test_losses = []\n",
        "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
      ],
      "metadata": {
        "id": "bxuXljOi-rzf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "С = -20\n",
        "# С = -0.5\n",
        "\n",
        "def train(epoch):\n",
        "  network.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "    # optimizer.zero_grad()\n",
        "    # inputs = torch.cat([data, target.reshape(-1, 1, 1, 1) * torch.ones_like(data) / 10.], dim=1).cuda()\n",
        "    # output = network(inputs)\n",
        "    # loss = F.nll_loss(output, target.cuda())\n",
        "    # loss.backward()\n",
        "    # optimizer.step()\n",
        "\n",
        "    # optimizer.zero_grad()\n",
        "    # inputs = torch.cat([data, -torch.ones_like(data)], dim=1).cuda()\n",
        "    # output = network(inputs)\n",
        "    # loss = F.nll_loss(output, target.cuda())\n",
        "    # loss.backward()\n",
        "    # optimizer.step()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    inputs_1 = torch.cat([data, С*torch.ones_like(data)], dim=1).cpu()\n",
        "\n",
        "    # t_inputs = (target + 0.1*torch.randn(target.shape)).reshape(-1, 1, 1, 1) * torch.ones_like(data)\n",
        "    t_inputs = target.reshape(-1, 1, 1, 1) * torch.ones_like(data)\n",
        "    inputs_2 = torch.cat([data, t_inputs], dim=1).cpu()\n",
        "\n",
        "    p = 0.5\n",
        "    mask = (torch.empty(inputs_1.shape[0], 1, 1, 1).uniform_(0, 1) > p).float().cpu()\n",
        "    inputs = inputs_1 * mask + inputs_2 * (1 - mask)\n",
        "    target = target\n",
        "\n",
        "    output = network(inputs, features=False)\n",
        "\n",
        "    loss = F.nll_loss(output, target.cpu()) #+ reg\n",
        "    # print(loss, reg)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss = loss.cpu()\n",
        "\n",
        "    if batch_idx % log_interval == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "        100. * batch_idx / len(train_loader), loss.item()))\n",
        "      train_losses.append(loss.item())\n",
        "      train_counter.append(\n",
        "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
        "      torch.save(network.state_dict(), '/vanilla_model.pth')\n",
        "      torch.save(optimizer.state_dict(), '/vanilla_optimizer.pth')\n",
        "\n",
        "def test():\n",
        "  network.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      # inputs = data.cuda()\n",
        "      inputs = torch.cat([data, С*torch.ones_like(data)], dim=1).cpu()\n",
        "      # inputs = (data + С*torch.ones_like(data)).cuda()\n",
        "      output = network(inputs).cpu()\n",
        "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  test_losses.append(test_loss)\n",
        "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(test_loader.dataset),\n",
        "    100. * correct / len(test_loader.dataset)))\n",
        "  return 100 * correct / len(test_loader.dataset)"
      ],
      "metadata": {
        "id": "lCugrW3f-xCw"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test()\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  train(epoch)\n",
        "  test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_IS4C6B_HMp",
        "outputId": "fab66caa-4b1f-4a04-a85d-04232c5e177e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-5adc203d06af>:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Avg. loss: 2.3181, Accuracy: 958/10000 (10%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.296962\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.279510\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.164294\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 1.984613\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 1.948497\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.690720\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 1.329213\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 1.178036\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.997572\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.543486\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.593093\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.702679\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.498126\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.414730\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.538169\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.324108\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.273956\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.315784\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.317373\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.264197\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.595801\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.203952\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.245353\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.204342\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.291177\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.199532\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.177710\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.222088\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.198038\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.126239\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.152459\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.290714\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.371905\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.223095\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.136252\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.117345\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.185326\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.290451\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.147148\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.082609\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.052640\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.138658\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.154724\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.198422\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.167832\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.150251\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.067553\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.104724\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.079496\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.014120\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.108365\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.086862\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.108238\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.230450\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.098640\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.050395\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.191555\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.061609\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.097528\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.298375\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.083868\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.035067\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.043053\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.087230\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.207376\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.075879\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.115513\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.099659\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.072725\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.120813\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.051573\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.126561\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.080941\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.086719\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.132762\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.072128\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.061739\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.047161\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.190183\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.060959\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.504727\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.102240\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.199445\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.043431\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.194550\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.171781\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.064246\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.075008\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.124981\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.070583\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.053441\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.078158\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.066763\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.073744\n",
            "\n",
            "Test set: Avg. loss: 0.1218, Accuracy: 9604/10000 (96%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.098386\n",
            "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.146106\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.103126\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.038075\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.094034\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.021194\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.138039\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.010799\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.048326\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.159297\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.048875\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.122317\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.036544\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.166892\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.108204\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.005615\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.037843\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.043566\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.076881\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.058515\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.051962\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.060551\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.037799\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.054760\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.011703\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.005304\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.058762\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.042174\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.017403\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.056344\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.018385\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.059933\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.084838\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.061983\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.066054\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.197249\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.024578\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.022301\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.048219\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.008928\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.043849\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.083623\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.087330\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.018451\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.037748\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.069975\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.073479\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.009755\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.059988\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.062941\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.056893\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.086087\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.095313\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.054746\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.003542\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.009319\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.054576\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.065936\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.035716\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.073102\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.076756\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.094622\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.062604\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.134994\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.071365\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.095543\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.033456\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.057460\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.078256\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.049472\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.120932\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.068334\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.025472\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.109428\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.082982\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.174073\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.148106\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.018071\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.008619\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.010530\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.031066\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.004451\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.156279\n",
            "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.032815\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.034707\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.011830\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.112106\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.044697\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.003215\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.079139\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.078455\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.052091\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.027374\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.091144\n",
            "\n",
            "Test set: Avg. loss: 0.0856, Accuracy: 9738/10000 (97%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.038121\n",
            "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.005813\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.067220\n",
            "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.099496\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.248503\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.036585\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.015594\n",
            "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.016107\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.039767\n",
            "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.027041\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.038477\n",
            "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.078923\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.125673\n",
            "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.057588\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.195933\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.111479\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.027836\n",
            "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.027789\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.025252\n",
            "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.034827\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.188032\n",
            "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.059571\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.006688\n",
            "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.047775\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.078476\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.049676\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.027038\n",
            "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.091557\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.047720\n",
            "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.137909\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.176196\n",
            "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.171768\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.027901\n",
            "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.041615\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.015864\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.098473\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.003947\n",
            "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.121559\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.002982\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.007377\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.015619\n",
            "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.112117\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.070698\n",
            "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.017981\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.028749\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.040693\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.017788\n",
            "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.018237\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.077048\n",
            "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.027740\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.023294\n",
            "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.211303\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.009634\n",
            "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.021983\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.041488\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.126011\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.067645\n",
            "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.033946\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.015168\n",
            "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.144690\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.045789\n",
            "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.062042\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.043824\n",
            "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.115470\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.027178\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.020494\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.101336\n",
            "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.069835\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.016455\n",
            "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.018594\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.007826\n",
            "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.032174\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.063309\n",
            "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.010907\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.192516\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.010408\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.008200\n",
            "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.063826\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.004224\n",
            "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.068201\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.057637\n",
            "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.033613\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.029200\n",
            "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.003549\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.022574\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.015296\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.055477\n",
            "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.052830\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.040355\n",
            "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.007858\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.037345\n",
            "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.079240\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.098624\n",
            "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.043812\n",
            "\n",
            "Test set: Avg. loss: 0.0636, Accuracy: 9808/10000 (98%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.016308\n",
            "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.017636\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.031230\n",
            "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.005663\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.033387\n",
            "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.006883\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.012309\n",
            "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.010922\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.128463\n",
            "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.042912\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.024481\n",
            "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.099980\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.026789\n",
            "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.110675\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.068756\n",
            "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.033386\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.060066\n",
            "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.009067\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.210634\n",
            "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.018358\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.056384\n",
            "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.227144\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.040618\n",
            "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.053698\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.022719\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.030268\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.003834\n",
            "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.012068\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.041317\n",
            "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.071850\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.055968\n",
            "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.081173\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.007029\n",
            "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.018034\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.032265\n",
            "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.160855\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.016940\n",
            "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.086693\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.001279\n",
            "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.034132\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.006051\n",
            "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.097211\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.193913\n",
            "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.017716\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.032158\n",
            "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.012703\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.008983\n",
            "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.058153\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.092142\n",
            "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.025948\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.045176\n",
            "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.031220\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.003871\n",
            "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.024692\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.086068\n",
            "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.004550\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.050036\n",
            "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.058163\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.004890\n",
            "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.073089\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.108682\n",
            "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.037922\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.049359\n",
            "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.054045\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.011291\n",
            "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.010997\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.002288\n",
            "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.016269\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.003412\n",
            "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.009766\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.005674\n",
            "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.106372\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.098134\n",
            "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.075965\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.069613\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.007156\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.164909\n",
            "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.048405\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.045462\n",
            "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.015461\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.008061\n",
            "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.029149\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.017729\n",
            "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.004545\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.012240\n",
            "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.025425\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.003794\n",
            "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.055943\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.029385\n",
            "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.038427\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.087183\n",
            "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.027936\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.052974\n",
            "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.069139\n",
            "\n",
            "Test set: Avg. loss: 0.0510, Accuracy: 9836/10000 (98%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.023280\n",
            "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.011849\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.002156\n",
            "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.028447\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.010048\n",
            "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.017426\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.059566\n",
            "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.006279\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.011644\n",
            "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.006886\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.004906\n",
            "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.008705\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.076482\n",
            "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.010303\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.092399\n",
            "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.007191\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.054553\n",
            "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.059342\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.004362\n",
            "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.004964\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.001277\n",
            "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.025141\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.058590\n",
            "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.035859\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.003690\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.008885\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.004987\n",
            "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.014690\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.010143\n",
            "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.019239\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.007650\n",
            "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.053353\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.047310\n",
            "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.088593\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.011539\n",
            "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.000738\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.058881\n",
            "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.009534\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.042857\n",
            "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.156965\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.109898\n",
            "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.072178\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.085560\n",
            "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.003297\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.054999\n",
            "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.103374\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.022295\n",
            "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.002783\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.066581\n",
            "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.017462\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.034238\n",
            "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.003597\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.106851\n",
            "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.029084\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.020586\n",
            "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.162767\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.039999\n",
            "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.029652\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.037644\n",
            "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.012880\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.051569\n",
            "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.017208\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.004947\n",
            "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.059139\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.035195\n",
            "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.046557\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.001861\n",
            "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.001501\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.058770\n",
            "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.000736\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.008171\n",
            "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.100335\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.021527\n",
            "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.002475\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.013795\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.001341\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.010627\n",
            "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.001620\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.032947\n",
            "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.086775\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.003161\n",
            "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.001897\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.055042\n",
            "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.080857\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.110003\n",
            "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.082769\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.020145\n",
            "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.029607\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.067691\n",
            "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.002662\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.052575\n",
            "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.022779\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.125039\n",
            "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.012550\n",
            "\n",
            "Test set: Avg. loss: 0.0498, Accuracy: 9836/10000 (98%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_fashion_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.FashionMNIST('/files/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_train, shuffle=True)\n",
        "\n",
        "test_fashion_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.FashionMNIST('/files/', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_test, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHdqYqiL_Hcw",
        "outputId": "2368a3ec-4bfb-46b2-93a2-b14993266574"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /files/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 15130826.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /files/FashionMNIST/raw/train-images-idx3-ubyte.gz to /files/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /files/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 267576.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /files/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /files/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /files/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 5038780.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /files/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /files/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /files/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 5940103.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /files/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /files/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing OOD metrics\n",
        "\n",
        "network.eval()\n",
        "\n",
        "uncertainties = np.array([])\n",
        "labels = np.array([])\n",
        "vectors = np.array([[]]).reshape([0, 10])\n",
        "vectors_2 = np.array([[]]).reshape([0, 10])\n",
        "\n",
        "C = -20\n",
        "N = 1\n",
        "eps = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for data, target in test_loader:\n",
        "    inputs = torch.cat([data, C*torch.ones_like(data)], dim=1).cpu()\n",
        "    # inputs = (data + C*torch.ones_like(data)).cuda()\n",
        "    output = network(inputs, features=True)\n",
        "    output = torch.softmax(output, dim=1)\n",
        "    pred_target = torch.argmax(output, dim=1)\n",
        "    # pred_input = pred_target\n",
        "    pred_input = output.topk(1).indices[:, -1]\n",
        "\n",
        "    for _ in range(N):\n",
        "      inputs = torch.cat([data.cpu(), pred_input.reshape(-1, 1, 1, 1) * torch.ones_like(data).cpu()], dim=1).cpu()\n",
        "      output_w = network(inputs, features=True)\n",
        "      output_w = torch.softmax(output_w, dim=1)\n",
        "      pred_target = torch.argmax(output_w, dim=1) + eps\n",
        "\n",
        "    # uncertainty = torch.abs(output - output_w).max(dim=1)[0].detach().cpu()\n",
        "    uncertainty = torch.abs(output - output_w).mean(dim=1).cpu()\n",
        "\n",
        "    label = np.zeros_like(uncertainty)\n",
        "\n",
        "    # vectors = np.concatenate([vectors, z_without.cpu()])\n",
        "    # vectors_2 = np.concatenate([vectors_2, z_with.cpu()])\n",
        "    uncertainties = np.concatenate([uncertainties, uncertainty])\n",
        "    labels = np.concatenate([labels, label])\n",
        "\n",
        "with torch.no_grad():\n",
        "  for data, target in test_fashion_loader:\n",
        "    inputs = torch.cat([data, C*torch.ones_like(data)], dim=1).cpu()\n",
        "    # inputs = (data + C*torch.ones_like(data)).cuda()\n",
        "    output = network(inputs, features=True)\n",
        "    output = torch.softmax(output, dim=1)\n",
        "    pred_target = torch.argmax(output, dim=1) + eps\n",
        "\n",
        "    # pred_input = pred_target\n",
        "    pred_input = output.topk(1).indices[:, -1]\n",
        "\n",
        "    for _ in range(N):\n",
        "      inputs = torch.cat([data.cpu(), pred_input.reshape(-1, 1, 1, 1) * torch.ones_like(data).cpu()], dim=1).cpu()\n",
        "      output_w = network(inputs, features=True)\n",
        "      output_w = torch.softmax(output_w, dim=1)\n",
        "      pred_target = torch.argmax(output_w, dim=1)\n",
        "\n",
        "    # uncertainty = torch.abs(output - output_w).max(dim=1)[0].detach().cpu()\n",
        "    uncertainty = torch.abs(output - output_w).mean(dim=1).cpu()\n",
        "\n",
        "    label = np.ones_like(uncertainty)\n",
        "\n",
        "    # vectors = np.concatenate([vectors, z_without.cpu()])\n",
        "    # vectors_2 = np.concatenate([vectors_2, z_with.cpu()])\n",
        "    uncertainties = np.concatenate([uncertainties, uncertainty])\n",
        "    labels = np.concatenate([labels, label])\n",
        "\n",
        "import sklearn.metrics\n",
        "roc_auc = sklearn.metrics.roc_auc_score(labels, uncertainties)\n",
        "precision, recall, thresholds = sklearn.metrics.precision_recall_curve(labels, uncertainties)\n",
        "pr_auc = sklearn.metrics.auc(recall, precision)\n",
        "\n",
        "print(f\"SINGLE MNIST ROC AUC: {roc_auc} PR AUC: {pr_auc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5Wx9sImAaen",
        "outputId": "5f297e33-6096-4d59-d67d-6f745eb1e3d6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-5adc203d06af>:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SINGLE MNIST ROC AUC: 0.9822305000000001 PR AUC: 0.9782993549537198\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "network.eval()\n",
        "\n",
        "uncertainties = np.array([])\n",
        "deltas = np.array([])\n",
        "losses = np.array([])\n",
        "labels = np.array([])\n",
        "\n",
        "vectors = np.array([[]]).reshape([0, 10])\n",
        "vectors_2 = np.array([[]]).reshape([0, 10])\n",
        "\n",
        "C = -20\n",
        "\n",
        "with torch.no_grad():\n",
        "  for data, target in test_loader:\n",
        "    # target = target.cuda()\n",
        "    target_onehot = torch.nn.functional.one_hot(target, 10).cpu()\n",
        "    inputs = torch.cat([data, C*torch.ones_like(data)], dim=1).cpu()\n",
        "    # inputs = (data + C*torch.ones_like(data)).cuda()\n",
        "    output = network(inputs, features=True)\n",
        "\n",
        "    nll_loss = F.nll_loss(output, target.cpu(), reduction='none').cpu()\n",
        "\n",
        "    pred_target = torch.argmax(output, dim=1)\n",
        "    inputs = torch.cat([data.cpu(), pred_target.reshape(-1, 1, 1, 1) * torch.ones_like(data).cpu()], dim=1).cpu()\n",
        "    # inputs = (data.cuda() + pred_target.reshape(-1, 1, 1, 1) * torch.ones_like(data).cuda()).cuda()\n",
        "    output_w = network(inputs, features=True)\n",
        "\n",
        "    # for delta compute\n",
        "    inputs = torch.cat([data.cpu(), target.cpu().reshape(-1, 1, 1, 1) * torch.ones_like(data).cpu()], dim=1).cpu()\n",
        "    output_t = network(inputs, features=True)\n",
        "\n",
        "    uncertainty = torch.abs(torch.nn.functional.softmax(output) - torch.nn.functional.softmax(output_w)).min(dim=1)[0].detach().cpu() #torch.abs(output - output_w).mean(dim=1).cpu()\n",
        "    delta = torch.abs(target_onehot - torch.nn.functional.softmax(output_t)).min(dim=1)[0].detach().cpu()\n",
        "\n",
        "    label = (pred_target.cpu() == target).cpu()\n",
        "\n",
        "    # vectors = np.concatenate([vectors, z_without.cpu()])\n",
        "    # vectors_2 = np.concatenate([vectors_2, z_with.cpu()])\n",
        "    uncertainties = np.concatenate([uncertainties, uncertainty])\n",
        "    deltas = np.concatenate([deltas, delta])\n",
        "    labels = np.concatenate([labels, label])\n",
        "    losses = np.concatenate([losses, nll_loss])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynUZL69iAhxq",
        "outputId": "c2342e9f-f871-4e39-9eaf-13ac991f321c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-5adc203d06af>:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n",
            "<ipython-input-35-10840cb8b1e3>:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  uncertainty = torch.abs(torch.nn.functional.softmax(output) - torch.nn.functional.softmax(output_w)).min(dim=1)[0].detach().cpu() #torch.abs(output - output_w).mean(dim=1).cpu()\n",
            "<ipython-input-35-10840cb8b1e3>:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  delta = torch.abs(target_onehot - torch.nn.functional.softmax(output_t)).min(dim=1)[0].detach().cpu()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "network.eval()\n",
        "\n",
        "uncertainties = np.array([])\n",
        "deltas = np.array([])\n",
        "labels = np.array([])\n",
        "errs = []\n",
        "\n",
        "vectors = np.array([[]]).reshape([0, 10])\n",
        "vectors_2 = np.array([[]]).reshape([0, 10])\n",
        "\n",
        "C = -20\n",
        "eps = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for data, target in test_loader:\n",
        "    # target = target.cuda()\n",
        "    target_onehot = torch.nn.functional.one_hot(target, 10).cpu()\n",
        "    inputs = torch.cat([data, C*torch.ones_like(data)], dim=1).cpu()\n",
        "    # inputs = (data + C*torch.ones_like(data)).cuda()\n",
        "    output = network(inputs, features=True)\n",
        "    pred_target = torch.argmax(output, dim=1)\n",
        "    # pred_input = (pred_target + eps)\n",
        "    pred_input = output.topk(1).indices[:, -1]\n",
        "    inputs = torch.cat([data.cpu(), pred_input.reshape(-1, 1, 1, 1) * torch.ones_like(data).cpu()], dim=1).cpu()\n",
        "    # inputs = (data.cuda() + pred_target.reshape(-1, 1, 1, 1) * torch.ones_like(data).cuda()).cuda()\n",
        "    output_w = network(inputs, features=True)\n",
        "\n",
        "    # for delta compute\n",
        "    inputs = torch.cat([data.cpu(), target.cpu().reshape(-1, 1, 1, 1) * torch.ones_like(data).cpu()], dim=1).cpu()\n",
        "    output_t = network(inputs, features=True)\n",
        "\n",
        "    # uncertainty = torch.abs(output - output_w).min(dim=1)[0].detach().cpu()\n",
        "    # uncertainty = torch.abs(torch.softmax(output, dim=1) - torch.softmax(output_w, dim=1)).max(dim=1)[0].detach().cpu()\n",
        "    uncertainty = torch.abs(torch.softmax(output, dim=1) - torch.softmax(output_w, dim=1)).mean(dim=1).detach().cpu()\n",
        "    delta = torch.abs(target_onehot - output_t).min(dim=1)[0].detach().cpu()\n",
        "\n",
        "    loss = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
        "    err = loss(output, pred_target.cpu()).tolist()\n",
        "    errs += err\n",
        "\n",
        "    label = (pred_target.cpu() == target).cpu()\n",
        "\n",
        "    # vectors = np.concatenate([vectors, z_without.cpu()])\n",
        "    # vectors_2 = np.concatenate([vectors_2, z_with.cpu()])\n",
        "    uncertainties = np.concatenate([uncertainties, uncertainty])\n",
        "    deltas = np.concatenate([deltas, delta])\n",
        "    labels = np.concatenate([labels, label])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7O6A123JA8uE",
        "outputId": "f907794e-ec89-4609-b543-e10ba764590a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-5adc203d06af>:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def AULC(accs, uncertainties):\n",
        "  idxs = np.argsort(uncertainties)\n",
        "  uncs_s = uncertainties[idxs]\n",
        "  error_s = accs[idxs]\n",
        "\n",
        "  mean_error = error_s.mean()\n",
        "  error_csum = np.cumsum(error_s)\n",
        "\n",
        "  Fs = error_csum / np.arange(1, len(error_s) + 1)\n",
        "  s = 1 / len(Fs)\n",
        "  return -1 + s * Fs.sum() / mean_error, Fs\n",
        "\n",
        "def rAULC(uncertainties, accs):\n",
        "    perf_aulc, Fsp = AULC(accs, -accs.astype(\"float\"))\n",
        "    curr_aulc, Fsc = AULC(accs, uncertainties)\n",
        "    print(perf_aulc, curr_aulc)\n",
        "    return curr_aulc / perf_aulc, Fsp, Fsc\n",
        "\n",
        "res, r1, r2 = rAULC(uncertainties, labels)\n",
        "print(res)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(range(len(r1)), r1)\n",
        "plt.plot(range(len(r1)), r2)\n",
        "plt.grid()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "vIL9b26JA9Wx",
        "outputId": "b1086715-baaf-4a9a-e28d-203cfff538e9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.01653513499589576 0.016018823067625476\n",
            "0.9687748586027002\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TkpFeJNJBcaA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}